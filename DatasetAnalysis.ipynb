{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product, islice\n",
    "from typing import Dict, List, Any, Tuple, Callable\n",
    "import os\n",
    "import pickle\n",
    "from timeit import default_timer as timer\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "from scipy.spatial import ConvexHull, Delaunay\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from IOData.IOData import InputRule\n",
    "from IOData.IODataWith_l import IODataWith_l\n",
    "\n",
    "from simulators.track_simulator import TrackSimulator\n",
    "from simulators.simulation_settings import SafetyFilterTypes, TrackFilterTypes, SimulationInputRule, ModelType\n",
    "from tools.simualtion_results import Results\n",
    "\n",
    "from tools.simple_track_generator import trackGenerator\n",
    "from tools.FractalDimension import fractal_dimension\n",
    "from tools.monte_carlo_integral import monte_carlo_integrate\n",
    "from tools.dataset_analyse import weighting_xi_in_datasets, weighting_u_pf_y_p_in_datasets, get_datasets_hankel_matrix, Sampler, from_deg_to_rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read results from file\n",
    "results_file_name = '09-20_14_17-results_list.pkl'\n",
    "with open(os.path.join(os.curdir, 'datasets', results_file_name), 'rb') as f:\n",
    "    results_list: List[Results] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name_list: List[str]  = [\n",
    "    '09-20-11-07-14-oval_track-INDIRECT_FIX_MU_WEIGHTING_ADD_DATA.pkl',\n",
    "    '09-20-11-08-43-oval_track-INDIRECT_FIX_MU_WEIGHTING_ADD_DATA.pkl',\n",
    "    '09-20-11-09-27-oval_track-INDIRECT_FIX_MU_WEIGHTING_ADD_DATA.pkl',\n",
    "    '09-20-11-09-44-oval_track-INDIRECT_FIX_MU_WEIGHTING_ADD_DATA.pkl',\n",
    "    '09-20-11-10-32-oval_track-INDIRECT_FIX_MU_WEIGHTING_ADD_DATA.pkl',\n",
    "    '09-20-11-10-42-oval_track-INDIRECT_FIX_MU_WEIGHTING_ADD_DATA.pkl',\n",
    "    '09-20-11-14-28-oval_track-INDIRECT_FIX_MU_WEIGHTING_ADD_DATA.pkl',\n",
    "    '09-20-11-18-57-oval_track-INDIRECT_FIX_MU_WEIGHTING_ADD_DATA.pkl',\n",
    "    '09-20-11-40-18-oval_track-INDIRECT_FIX_MU_WEIGHTING_ADD_DATA.pkl',\n",
    "    '09-20-11-40-41-oval_track-INDIRECT_FIX_MU_WEIGHTING_ADD_DATA.pkl',\n",
    "    '09-20-11-48-02-oval_track-INDIRECT_FIX_MU_WEIGHTING_ADD_DATA.pkl',\n",
    "    '09-20-11-50-16-oval_track-INDIRECT_FIX_MU_WEIGHTING_ADD_DATA.pkl',\n",
    "    '09-20-12-43-52-oval_track-INDIRECT_FIX_MU_WEIGHTING_ADD_DATA.pkl',\n",
    "    '09-20-12-58-02-oval_track-INDIRECT_FIX_MU_WEIGHTING_ADD_DATA.pkl',\n",
    "    '09-20-14-08-04-oval_track-INDIRECT_FIX_MU_WEIGHTING_ADD_DATA.pkl',\n",
    "    '09-20-14-10-34-oval_track-INDIRECT_FIX_MU_WEIGHTING_ADD_DATA.pkl',\n",
    "    '09-20-14-15-40-oval_track-INDIRECT_FIX_MU_WEIGHTING_ADD_DATA.pkl',\n",
    "    '09-20-14-17-14-oval_track-INDIRECT_FIX_MU_WEIGHTING_ADD_DATA.pkl',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[539, 523, 503]\n",
      "[]\n",
      "[551, 535, 515]\n",
      "[159, 143, 123]\n",
      "[531, 515, 495]\n",
      "[55, 39, 19]\n",
      "[55, 39, 19]\n",
      "[547, 531, 511]\n",
      "[571, 555, 535]\n",
      "[151, 135, 115]\n",
      "[551, 535, 515]\n",
      "[151, 135, 115]\n",
      "[687, 671, 651]\n",
      "[551, 535, 515]\n",
      "[543, 527, 507]\n",
      "[679, 663, 643]\n",
      "[]\n",
      "[943, 927, 907]\n",
      "531\n"
     ]
    }
   ],
   "source": [
    "for result in results_list:\n",
    "    print(result.mark_time_steps)\n",
    "    # print([traj_slice[0] for traj_slice in result._error_trajectory_slices])\n",
    "print(round(5.32/0.01)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# parameters for analysis stored in Sampler\n",
    "sampler = Sampler()\n",
    "sampler.l_p = 10\n",
    "sampler.l_f = 85\n",
    "\n",
    "# results and dataset index to analyse\n",
    "results_index = range(3, 18, 3)\n",
    "dataset_list: List[List[List[IODataWith_l]]] = []\n",
    "for i in results_index:\n",
    "    with open(os.path.join(os.curdir, 'datasets', dataset_name_list[i]), 'rb') as f:\n",
    "        single_dataset_list: List[List[IODataWith_l]] = pickle.load(f)\n",
    "    dataset_list.append(single_dataset_list)\n",
    "\n",
    "# get output and input size\n",
    "m = dataset_list[0][0][0]._input_data[0].shape[0]\n",
    "p = dataset_list[0][0][0]._output_data[0].shape[0]\n",
    "\n",
    "# extended state to analyse\n",
    "state_index = 159\n",
    "results = results_list[results_index[0]]\n",
    "t_proposed = (state_index+1) * results.Ts\n",
    "u_list, y_list = [], []\n",
    "for i in range(state_index-sampler.l_p+1, state_index+1):\n",
    "    u_list.append(results._input_applied[i])\n",
    "    y_list.append(from_deg_to_rad(results._error_trajectory[i][:p]))\n",
    "xi_to_analyse = np.hstack(u_list + y_list)\n",
    "# print(xi_to_analyse)\n",
    "\n",
    "# extended state with future input to analyse. use the \n",
    "u_future_list = []\n",
    "index_proposed = [trajectory_slice[0] for trajectory_slice in results._proposed_input_slices].index(t_proposed)\n",
    "list_proposed_u = results._proposed_input_slices[index_proposed][1]\n",
    "for i in range(sampler.l_f):\n",
    "    u_future_list.append(list_proposed_u[i])\n",
    "u_pf_y_p_to_analyse = np.hstack(u_list + u_future_list + y_list)\n",
    "# print(u_pf_y_p_to_analyse)\n",
    "\n",
    "# index of segment to analyse\n",
    "i_seg = results._segment_index_list[state_index]\n",
    "print(i_seg)\n",
    "\n",
    "results_list_to_analyse = [results_list[i] for i in results_index]\n",
    "dataset_list_to_analyse = dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1648, 6)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "lag_fractal = 1\n",
    "L_fractal = 0\n",
    "dataset_list_for_seg = dataset_list_to_analyse[0][i_seg]\n",
    "H_uy_noised, _ = get_datasets_hankel_matrix(dataset_list_for_seg, lag=lag_fractal, L=L_fractal)\n",
    "locs = np.array(H_uy_noised[:].T)\n",
    "print(locs.shape)\n",
    "\n",
    "region_min = np.min(locs, axis=0)\n",
    "region_max = np.max(locs, axis=0)\n",
    "\n",
    "d, log_N, log_inverse_size = fractal_dimension(locs=locs, region_min=region_min, region_max=region_max, max_box_size=0, min_box_size=-6, n_samples=20, plot=True)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, log_N, log_inverse_size = fractal_dimension(locs=locs, region_min=region_min, region_max=region_max, max_box_size=0, min_box_size=-8, n_samples=20, plot=True)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000, 196, 140, 140, 156, 16]\n",
      "The weight of xi: 0.5343227185316545\n",
      "The weight of u_pf_y_p: 0.1503372201405862\n",
      "Value for xi integral: [10.441109660786786], error: [0.2429260601474776]\n",
      "Value for u_pf_y_p integral: [1.4963944420000264], error: [0.014110657244999122]\n",
      "--------------------------------------------------\n",
      "[1000, 196, 140, 140, 156, 16, 140]\n",
      "The weight of xi: 0.6211660704541004\n",
      "The weight of u_pf_y_p: 0.15547325190514194\n",
      "Value for xi integral: [10.563493443181079], error: [0.26229875338462333]\n",
      "Value for u_pf_y_p integral: [1.5477281677016292], error: [0.015430647038789096]\n",
      "--------------------------------------------------\n",
      "[1000, 196, 140, 140, 156, 16, 140, 144, 156]\n",
      "The weight of xi: 0.7586601409652871\n",
      "The weight of u_pf_y_p: 0.16697486082923246\n",
      "Value for xi integral: [12.249214571873738], error: [0.340375716151949]\n",
      "Value for u_pf_y_p integral: [1.7178654549297356], error: [0.01676351449537199]\n",
      "--------------------------------------------------\n",
      "[1000, 196, 140, 140, 156, 16, 140, 144, 156, 144, 4, 160]\n",
      "The weight of xi: 0.8950500064918747\n",
      "The weight of u_pf_y_p: 0.178730703170735\n",
      "Value for xi integral: [12.368877156191378], error: [0.3505073144318904]\n",
      "Value for u_pf_y_p integral: [1.912363372228032], error: [0.019627321815921493]\n",
      "--------------------------------------------------\n",
      "[1000, 196, 140, 140, 156, 16, 140, 144, 156, 144, 4, 160, 156, 140, 160]\n",
      "The weight of xi: 1.0\n",
      "The weight of u_pf_y_p: 0.196104824216016\n",
      "Value for xi integral: [14.813818745117171], error: [0.37294674376323883]\n",
      "Value for u_pf_y_p integral: [2.109867987003155], error: [0.02147129933146124]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# parameters used for analysing dataset\n",
    "\n",
    "size_xi = len(xi_to_analyse)\n",
    "size_uy = len(u_pf_y_p_to_analyse)\n",
    "\n",
    "W_xi = np.matrix(np.eye(size_xi))\n",
    "W_uy = np.matrix(np.eye(size_uy))\n",
    "f = lambda x: 0.01/x\n",
    "\n",
    "lag = 10\n",
    "L = 85\n",
    "\n",
    "weighting_xi_list: List[float] = []\n",
    "weighting_uy_list: List[float] = []\n",
    "for dataset_list_for_track in dataset_list:\n",
    "    dataset_list_for_seg = dataset_list_for_track[i_seg]\n",
    "    print([data.length for data in dataset_list_for_seg])\n",
    "\n",
    "    # evalueate the weighting matrix for xi\n",
    "    weighting_xi = weighting_xi_in_datasets(W_xi=W_xi, f=f, io_data_list=dataset_list_for_seg, lag=lag, L=L, l_p=sampler.l_p, l_f=sampler.l_f, xi=xi_to_analyse)\n",
    "    weighting_xi_list.append(weighting_xi)\n",
    "    print(f\"The weight of xi: {weighting_xi}\")\n",
    "\n",
    "    # evalueate the weighting matrix for u_pf_y_p\n",
    "    weighting_uy = weighting_u_pf_y_p_in_datasets(W=W_uy, f=f, io_data_list=dataset_list_for_seg, lag=lag, L=L, l_p=sampler.l_p, l_f=sampler.l_f, u_pf_y_p=u_pf_y_p_to_analyse)\n",
    "    weighting_uy_list.append(weighting_uy)\n",
    "    print(f\"The weight of u_pf_y_p: {weighting_uy}\")\n",
    "\n",
    "    # try monte carlo integral\n",
    "    # start = timer()\n",
    "    values = []\n",
    "    errors = []\n",
    "    for _ in range(1):\n",
    "        integrand = lambda x: weighting_xi_in_datasets(W_xi=W_xi, f=f, io_data_list=dataset_list_for_seg, lag=lag, L=L, l_p=sampler.l_p, l_f=sampler.l_f, xi=x)\n",
    "        value, error = monte_carlo_integrate(integrand, sampler.xi_iterator(), 100)\n",
    "        values.append(value)\n",
    "        errors.append(error)\n",
    "    print(f\"Value for xi integral: {values}, error: {errors}\")\n",
    "    # end = timer()\n",
    "    # print(f\"Time for xi integral: {end-start}\")\n",
    "\n",
    "    # start = timer()\n",
    "    values = []\n",
    "    errors = []\n",
    "    for _ in range(1):\n",
    "        integrand = lambda x: weighting_u_pf_y_p_in_datasets(W=W_uy, f=f, io_data_list=dataset_list_for_seg, lag=lag, L=L, l_p=sampler.l_p, l_f=sampler.l_f, u_pf_y_p=x)\n",
    "        value, error = monte_carlo_integrate(integrand, sampler.u_pf_y_p_iterator(), 100)\n",
    "        values.append(value)\n",
    "        errors.append(error)\n",
    "    print(f\"Value for u_pf_y_p integral: {values}, error: {errors}\")\n",
    "    # end = timer()\n",
    "    # print(f\"Time for u_pf_y_p integral: {end-start}\")\n",
    "    print(\"--------------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DDSF_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
